{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(r\"/home/ADF/axs1603/test_runs/cloud_detector_simstest/pics\")\n",
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "print(image_count)\n",
    "\n",
    "\n",
    "#generating a cloudy pic\n",
    "cloudy = list(data_dir.glob('cloudy/*'))\n",
    "PIL.Image.open(str(cloudy[13]))\n",
    "\n",
    "#generating a clear sky pic\n",
    "clear = list(data_dir.glob('clear/*'))\n",
    "PIL.Image.open(str(clear[13]))\n",
    "\n",
    "\n",
    "#defining some parameters\n",
    "\n",
    "batch_size = 64\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "\n",
    "#make a training set with 80-20 split\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split = 0.2,\n",
    "    subset = \"training\",\n",
    "    seed = 17,\n",
    "    image_size = (img_height,img_width),\n",
    "    batch_size = batch_size)\n",
    "\n",
    "\n",
    "#make a validation set \n",
    "vals_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed = 17,\n",
    "    image_size=(img_height,img_width),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "\n",
    "#verifying by printing class names\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)\n",
    "\n",
    "#visualising the data\n",
    "plt.figure(figsize=(10,10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3,3,i+1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        \n",
    "#load data using dataset.cache() to prevent blocking I/O\n",
    "#use Dataset.prefetch() to overlap data preprocessing and model execution\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_df = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "vals_ds = vals_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "#standardise/normalise the data\n",
    "\n",
    "normalization_layer = layers.experimental.preprocessing.Rescaling(1./255)\n",
    "\n",
    "\n",
    "\n",
    "#Applying data augmentation to improve predictions \n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "[layers.experimental.preprocessing.RandomFlip(\"horizontal\",\n",
    "                                             input_shape = (img_height, \n",
    "                                                           img_width,\n",
    "                                                           3)),\n",
    "layers.experimental.preprocessing.RandomRotation(0.4),\n",
    "layers.experimental.preprocessing.RandomZoom(0.3)])\n",
    "\n",
    "#visualise augmented pics\n",
    "plt.figure(figsize=(10,10))\n",
    "for images, _ in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        augmented_images = data_augmentation(images)\n",
    "        ax = plt.subplot(3,3,i+1)\n",
    "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        \n",
    "#Applying dropout to improve regularisation, and creating the model \n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "model = Sequential([\n",
    "    data_augmentation,\n",
    "    layers.experimental.preprocessing.Rescaling(1./255),\n",
    "    layers.Conv2D(16, 3, padding = 'same', activation = 'softmax'), #first layer has 16 filters\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(32, 3, padding = 'same', activation = 'softmax'), #second layer has 32 filters\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, padding = 'same', activation = 'softmax'), #third layer has 64 filters\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.2), # drop out 20% output units\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation = 'relu'),\n",
    "    layers.Dense(num_classes)    \n",
    "])\n",
    "\n",
    "#compiling and training the model\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "             loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "             metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "epochs = 15\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=vals_ds,\n",
    "    epochs=epochs)\n",
    "\n",
    "#visualise the results\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#testing with a new picture\n",
    "    #using cloudy picture\n",
    "    \n",
    "test_path = Path(r\"/home/ADF/axs1603/test_runs/cloud_detector_simstest/AllSkyImage20190701-202200.jpg\")\n",
    "img = keras.preprocessing.image.load_img(\n",
    "    test_path, target_size = (img_height, img_width))\n",
    "\n",
    "img_array = keras.preprocessing.image.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) \n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\"This image most likely belongs to {} with {:.2f} % confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
